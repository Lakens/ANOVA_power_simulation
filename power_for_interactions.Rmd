---
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Power for interactions.

```{r, message=FALSE, warning=FALSE}
# Install the two functions from GitHub by running the code below:

source("ANOVA_design.R")
source("ANOVA_power.R")

###############
# Load libraries ----
###############

library(mvtnorm)
library(afex)
library(lsmeans)
library(ggplot2)
library(gridExtra)
library(reshape2)
```

It is common in psychological science to examine studies with multiple factors, and analyse these results in an Analysis of Variance (ANOVA). For example, we might perform a study where we give students a an intermittent exam, or not, and a short quiz at the beginning of each lecture, or not. We might want to examine whether the effects of both these interventions (an intermittant exam, and quizzes) are additive, or not. Two possible hypotheses are that we might predict two main effects, with higher final grades when quizzes are given, and when an intermittent exam is presented, but both benefits stack and thus grades are best when both  

```{r, echo=FALSE, message=FALSE, warning=FALSE}
mu <- c(1, 2, 3, 4)
sd <- 1
n <- 20

df_means <- data.frame(mu, SE = sd / sqrt(n))

  for(j in 1:factors){
    df_means <- cbind(df_means, as.factor(unlist(rep(as.list(paste(letters[[j]], 
                                                                   1:as.numeric(strsplit(string, "\\D+")[[1]])[j], 
                                                                   sep="")), 
                                                     each = prod(as.numeric(strsplit(string, "\\D+")[[1]]))/prod(as.numeric(strsplit(string, "\\D+")[[1]])[1:j]),
                                                     times = prod(as.numeric(strsplit(string, "\\D+")[[1]]))/prod(as.numeric(strsplit(string, "\\D+")[[1]])[j:factors])
    ))))
  }
  
  if(factors == 1){names(df_means)<-c("mu","SE","a")}
  if(factors == 2){names(df_means)<-c("mu","SE","a","b")}
  if(factors == 3){names(df_means)<-c("mu","SE","a","b","c")}
  
  if(factors == 1){meansplot = ggplot(df_means, aes(y = mu, x = a))}
  if(factors == 2){meansplot = ggplot(df_means, aes(y = mu, x = a, fill=b))}
  if(factors == 3){meansplot = ggplot(df_means, aes(y = mu, x = a, fill=b)) + facet_wrap(  ~ c)}
  
  meansplot = meansplot +
    geom_bar(position = position_dodge(), stat="identity") +
    geom_errorbar(aes(ymin = mu-SE, ymax = mu+SE), 
                  position = position_dodge(width=0.9), size=.6, width=.3) +
    coord_cartesian(ylim=c((.7*min(mu)), 1.2*max(mu))) +
    theme_bw() + ggtitle("Means for each condition in the design")
  print(meansplot)  


```

## The ANOVA_power function

The ANOVA_power function takes the result from the ANOVA_design function, and simulates data nsims times. As output, it provides a table for the ANOVA results, and the results for contrasts. 

At least on windows systems, a progress bar should appear that shows the progress for the simulation. Larger numbers yield more accurate results, but also take a long time. I recommend testing with a minimum of 1000 simulations, or 10000 if you are getting a coffee. 

## An Example

In the example below, 1000 simulations for a 2*2 mixed design (first factor between, second factor within) is performed. The sample size is 80 in each between subject condition (so 160 participants in total), the sd is 1.03, the correlation for the within factors is 0.87, and the means are 1.03, 1.21, 0.98, 1.01. No correction for multiple comparisons is made.

The alpha level used as a significance threshold can be specified, and is set to 0.05 for this simulation.

```{r}
design_result <- ANOVA_design(string = "2b*2w",
                   n = 80, 
                   mu = c(1.03, 1.21, 0.98, 1.01), 
                   sd = 1.03, 
                   r=0.87, 
                   p_adjust = "none")

simulation_result <- ANOVA_power(design_result, alpha = 0.05, nsims = 1000)

```

The result for the power simulation has two sections. The first table provides power (from 0 to 100%) and effect sizes (partial eta-squared) for the ANOVA result. We see the results for the main effects of factor a, b and the interaction between a and b. 

The result for the power simulation reveal power is very high for the main effect of b - remember that this is the within-subjects factor, and the means are highly correlated (0.87) - so we have high power for within comparisons. Power is lower for the interaction. 

An ANOVA is typically followed up with contrasts. A statistical hypothesis often predicts not just an interaction, but also the shape of an interaction. For example, when looking at the plot of our design above, we might be specifically interested in comparing the mean in condition a1,b2 against a1,b1 and a2,b2 in simple contrasts. 

The second table provides the power for all contrasts, and the effect sizes. Effect sizes are provided in Cohen's d for between-subject contrasts, and in Cohen's dz for within-subject contrasts (see Lakens, 2013). These are the effect sizes used in a-priori power analysis. Note that Cohen's d is slightly upwardly biased when calculated from observed data (as in these simulations).

Power is relatively high for the contrast comparing a1,b2-a1,b1 - remember this is the within-subject contrast where means differ, and the correlation between dependent observations is large (r = 0.87). Power for the contrast a1,b2-a2,b2 is much lower, because this is a between subjects comparison. 

Power is very low for the minor differences among the three similar means (1.03, 0.98, 1.01) as can be seen from first, third, and fifth lines in the contrast output. 

Note the difference in the effect size estimates. For the contrast a1,b1 - a1,b2 Cohen's dz is much larger (due to the strong positive correlation) than Cohen's d reported for the contrast a1,b2 - a2,b2, even though the raw mean differences are almost identical. This is because Cohen's dz takes the correlation into account. 

In addition to the two tables, the ANOVA_power function returns the raw simulation data (all p-values and effect sizes for each simulation, use simulation_result$sim_data) and a plot showing the p-value distributions for all tests in the ANOVA.

```{r}
simulation_result$plot1
```

