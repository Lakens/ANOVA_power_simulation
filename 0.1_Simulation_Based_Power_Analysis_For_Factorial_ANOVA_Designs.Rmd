---
title             : "Simulation-Based Power-Analysis for Factorial ANOVA Designs"
shorttitle        : "ANOVA Power"
author: 
  - name          : "Dani&euml;l Lakens"
    affiliation   : "1"
    corresponding : yes
    address       : "ATLAS 9.402, 5600 MB, Eindhoven, The Netherlands"
    email         : "D.Lakens@tue.nl"
  - name          : "Aaron R. Caldwell"
    affiliation   : "2"
affiliation:
  - id            : "1"
    institution   : "Human-Technology Interaction Group, Eindhoven University of Technology, The Netherlands"
  - id            : "2"
    institution   : "Department of Health, Human Performance and Recreation, University of Arkansas, USA"
abstract: |
  Researchers often rely on analysis of variance (ANOVA) when they report results of experiments. To ensure a study is adequately powered to yield informative results when performing an ANOVA, researcher can perform an a-priori power analysis. However, power analysis for factorial ANOVA designs is often a challenge. Current software solutions do not enable power analyses for complex designs with several within-subject factors. Moreover, power analyses often need partial eta-squared or Cohen's f as input, but these effect sizes are not intuitive and do not generalize to different experimental designs. We have created the R package ANOVApower and an online Shiny app to enable researchers without extensive programming experience to perform simulation-based power analysis for ANOVA designs of up to three within- or between-subject factors, with an unlimited number of levels. Predicted effects are entered by specifying means, standard deviations, and correlations (for within-subject factors). The simulation provides the statistical power for all ANOVA main effects, interactions, and individual comparisons, and allow researchers to correct for multiple comparisons. The simulation plots *p*-value distributions for all tests, and power plots across a range of sample sizes. This tutorial will demonstrate how to perform power analysis for ANOVA designs, and highlights important factors that determine the statistical power of factorial ANOVA designs.
  
keywords          : "power analysis, ANOVA, hypothesis test, sample size justification, repeated measures"
wordcount         : 4654 words.
bibliography      : ["anova_power.bib"]
figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
mask              : no
class             : "jou"
output            : papaja::apa6_pdf
editor_options: 
  chunk_output_type: console
---
```{r load_packages, include=FALSE}
#devtools::install_github("Lakens/ANOVApower")
library(ANOVApower)
library(ggplot2)

#Set to low number when test-compiling, set to 100.000 for final manuscript.
nsims <- 100000

#Set manuscript seed

manuscript_seed = 2019

```

When a researcher aims to test hypotheses with an analysis of variance (ANOVA), the sample size of the study should be justified based on the statistical power of the test. 
The statistical power of a test is the probability of rejecting the null-hypothesis, given a specified effect size, alpha level, and sample size. 
When power is low there is a high  probability of concluding there is no effect when the alternative hypothesis is true.
Several excellent resources exist that explain power analyses, including books [@aberson_applied_2019; @cohen_statistical_1988], general reviews [@maxwell_sample_2008], and practical primers [@brysbaert_how_2019; @perugini_practical_2018]. 
Whereas power analyses for individual comparisons are relatively easy to perform, power analyses for factorial ANOVA designs are a bigger challenge. 
Available software solutions do not provide easy options to specify more complex designs (e.g., a 2x2x2 design, where the first factor is manipulated between participants, and the last two factors are manipulated within participants). 
The predicted effects often need to be specified as Cohen's f or partial eta squared ($\eta_p^2$), which are not the most intuitive way to specify a hypothesized pattern of results, and these effect sizes do not generalize to different experimental designs.
Simulations based on a specified pattern of means and a covariance matrix (based on the expected standard deviation and correlation between within participant factors) provide a more flexible approach to power analyses. 
However, such simulations typically require extensive programming knowledge.

In this manuscript we introduce ANOVApower, an R package and Shiny app that can be used to perform power analyses for factorial ANOVA designs based on simulations. 
ANOVApower can be used to calculate the statistical power based on a predicted pattern of means, standard deviations, and correlations (for within-subject factors).
By simulating data for factorial designs with specific parameters researchers can gain a better understanding of the factors that determine the statistical power of an ANOVA, and learn how to design well-powered experiments.
After a short introduction to statistical power, focussing on the *F*-test, we will illustrate through simulations how the power of factorial ANOVA designs depend on the pattern of means across conditions, the number of factors and levels, the sample size, and whether you need to control the alpha level for multiple comparisons.

# Calculating Power in ANOVA Designs

Imagine you plan to perform a study in which participants interact with an artificial voice assistant who sounds either cheerful or sad.
You measure how much 80 participants in each condition enjoy to interact with the voice assistant on a line marking scale (coded continuously from -5 to 5) and observe a mean of 0 in the sad condition, and a means of 1 in the cheerful condition, with an estimated standard deviation of 2.
After submitting your manuscript for publication, reviewers ask you to add a study with a neutral control condition to examine whether cheerful voices increase, or sad voices decrease enjoyment (or both). 
Depending on what the mean enjoyment in the neutral condition is, what sample size would you need to collect for a high powered test of the expected pattern of means? 
A collaborator suggests to switch from a between-subject design to a within-subject design to collect data more efficiently.
What impact will switching from a between-subject to a within-subject design have on the required sample size?
The effect size in the first study could be considered 'medium' based on the benchmarks by @cohen_statistical_1988, but does it make sense to plan for a 'medium' effect size in either the between-subject or within-subject ANOVA design?
And if you justify the sample size based on the power for the main effect for the ANOVA, will the study also have sufficient statistical power for the independent comparisons between conditions (or vice versa)?

Let's consider the initial study described above, where enjoyment is measured when 80 participants per condition interact with a cheerful or sad voice assistant.
We can test the difference between two means with a *t*-test or a one-way ANOVA, and the two tests are mathematically equivalent. 
Figure \ref{fig:d-plot} and Figure \ref{fig:eta-plot} visualize the distribution of the effect sizes Cohen's d (for the *t*-test) and $\eta_p^2$ (for the *F*-test) that should be observed when there is no effect (grey curves) and when the observed difference between means equals the true effect (black curves).
In both figures the light grey areas under the null-distribution mark the observed results that would lead to a Type 1 error (observing a statistically significant result if the null-hypothesis is true) and the dark grey areas under the curve marks the observed effect sizes that would lead to a Type 2 error (observing a non-significant result when there is  true effect).

```{r d-plot, fig.width=7, fig.height=4, echo=FALSE, message=FALSE, warning=FALSE, results='hide', fig.cap="Distribution of Cohen's d under the null-hypothesis (grey curve) and alternative hypothesis assuming d = 0.5 (black curve)."}
    N <- 80
    d <- 0.5
    p_upper<-.05
    p_lower<-0
    ymax<-25 #Maximum value y-scale (only for p-curve)
    
    ncp<-(d*sqrt(N/2)) #Calculate non-centrality parameter d
    low_x<--1
    high_x<-1.5

    #calc d-distribution
    x=seq(low_x,high_x,length=10000) #create x values
    d_dist<-dt(x*sqrt(N/2),df=(N*2)-2, ncp = ncp)*sqrt(N/2) #calculate distribution of d based on t-distribution
    #Set max Y for graph
    y_max<-max(d_dist)+1
    #create plot
    par(bg = "white")
    plot(-10,xlim=c(low_x,high_x), ylim=c(0,y_max), xlab=substitute(paste("Cohen's ", delta)), ylab="Density",main="")
    axis(side = 1, at = seq(low_x,high_x,0.1), labels = FALSE)
    lines(x,d_dist,col='black',type='l', lwd=2)
    #add d = 0 line
    d_dist<-dt(x*sqrt(N/2),df=(N*2)-2, ncp = 0)*sqrt(N/2)
    lines(x,d_dist,col='grey',type='l', lwd=2)
    #Add Type 1 error rate right
    crit_d<-abs(qt(p_upper/2, (N*2)-2))/sqrt(N/2)
    y=seq(crit_d,10,length=10000) 
    z<-(dt(y*sqrt(N/2),df=(N*2)-2)*sqrt(N/2)) #determine upperbounds polygon
    polygon(c(crit_d,y,10),c(0,z,0),col="lightgrey")
    #Add Type 1 error rate left
    crit_d<--abs(qt(p_upper/2, (N*2)-2))/sqrt(N/2)
    y=seq(-10, crit_d, length=10000) 
    z<-(dt(y*sqrt(N/2),df=(N*2)-2)*sqrt(N/2)) #determine upperbounds polygon
    polygon(c(y,crit_d,crit_d),c(z,0,0),col="lightgrey")
    #Add Type 2 error rate
    crit_d<-abs(qt(p_upper/2, (N*2)-2))/sqrt(N/2)
    y=seq(-10,crit_d,length=10000) 
    z<-(dt(y*sqrt(N/2),df=(N*2)-2, ncp=ncp)*sqrt(N/2)) #determine upperbounds polygon
    polygon(c(y,crit_d,crit_d),c(0,z,0),col="darkgrey")
    segments(crit_d, 0, crit_d, y_max-0.8, col= 'black', lwd=2, lty = "dashed")
    segments(-crit_d, 0, -crit_d, y_max-0.8, col= 'black', lwd=2, lty = "dashed")
```

```{r eta-plot, fig.width=7, fig.height=4, echo=FALSE, message=FALSE, warning=FALSE, results='hide', fig.cap="Distribution of eta-squared under the null-hypothesis (grey curve) and alternative hypothesis assuming partial eta-squared = 0.0588 (black curve)."}

    N<-80
    J<-2
    eta_pop<-0.0588
    alpha<-.05
    xmax<-.2
    ymax<-.6
    
    xmin <- 0
    #Calculations
    df1 <- J-1
    df2 <- J*(N - 1)
    Ntot = N*J
    ncp = Ntot/(1/eta_pop-1)
    crit_f <- qf(1 - alpha, df1, df2)
    
    x=seq(xmin,xmax,length=1000)
    #F-value function
    eta_pop_dist <- function(x) df((x*df2)/(df1-x*df1), df1, df2, ncp)
    par(bg = "white")
    plot(-10,xlab=substitute(paste(eta[p]^2)), ylab="Density", axes=FALSE,
         main = "", xlim=c(0,xmax),  ylim=c(0, ymax))
    #title in shiny app: main=substitute(paste("distribution for ",eta[p]^2 == eta_pop," for ",J," groups and ", N, " observations per group."
    axis(side=1, at=seq(0,xmax, 0.02), labels=seq(0,xmax, 0.02))
    axis(side=2)
    ncp<-0 #set ncp to 0 to plot the F distribution for null effect.
    eta_pop_crit<-(crit_f*df1)/(crit_f*df1+df2)
    
    curve(eta_pop_dist, 0.00000000001, 1, n=10000, col="grey", lwd=2, add=TRUE)
    x=seq(eta_pop_crit,xmax,length=10000) 
    z<-df((x*df2)/(df1-x*df1), df1, df2) #determine upperbounds polygon
    polygon(c(eta_pop_crit,x,1),c(0,z,0),col="lightgray") #draw polygon
    #Add Type 2 error rate
    ncp = Ntot/(1/eta_pop-1)
    curve(eta_pop_dist, 0.00000000001, 1, n=10000, col="black", lwd=2, add=TRUE)
    y=seq(0.00000000001,eta_pop_crit,length=10000) 
    z<-df((y*df2)/(df1-y*df1), df1, df2, ncp) #determine upperbounds polygon
    polygon(c(y,eta_pop_crit,eta_pop_crit),c(0,z,0),col="darkgray")
    segments(eta_pop_crit, 0, eta_pop_crit, ymax-0.03, col= 'black', lwd=2, lty = "dashed")
    
```

When the observed data is more extreme than a critical value, the test result is statistically significant.
Critical values are often expressed as *t*-values or *F*-values, but for a given sample size they can also be expressed as critical *effect sizes*.
Given the sample size of 80 participants per group, observed effects are statistically significant when they are larger than d = `r crit_d` in a *t*-test, or $\eta_p^2$ = `r eta_pop_crit` for the *F*-test.
The goal of an a-priori power analysis is to choose a sample size for which the probability of observing a statistically significant effect reaches a desired probability. 
To calculate power, one has to specify the alternative hypothesis (the black curves in Figure \ref{fig:d-plot} and \ref{fig:eta-plot}).
If we assume that under the alternative hypothesis the true effect size is d = 0.5 or $\eta_p^2$ = 0.0588, and data is collected from 80 participants in each condition, in the long run `r round(100*power_oneway_between(ANOVA_design(design = "2b", 80, c(1,0), 2, 0, c("condition", "cheerful", "sad"), FALSE))$power, 2)`% of the observed data will yield statistically significant results.

The *t*-test examines the difference between means $(m_1 - m_2)$, and the *F*-test computes the ratio of the between group variance and the within group variance. 
For two groups, the variance of the difference between two means is $(m_1 - m_2)^2$, and therefore $F = t^2$.
Cohen's d is calculated by dividing the difference between means by the standard deviation, or 
\begin{equation}
d = \frac{m_1-m_2}{\sigma}.
\end{equation}
If we have two groups with means of 0 and 1, and the standard deviation is 2, Cohen's d is (1-0)/2, or 0.5.
The generalization of Cohen's d to more than two groups is Cohen's f, which is the standard deviation of the population means divided by the population standard deviation [@cohen_statistical_1988], or: 
\begin{equation}
f = \frac{\sigma _{ m }}{\sigma}
\end{equation}
where for equal sample sizes,
\begin{equation}
\sigma _{ m } = \sqrt { \frac { \sum_ { i = 1 } ^ { k } ( m _ { i } - m ) ^ { 2 } } { k } }.
\end{equation}
For two groups Cohen's f is half as large as Cohen's d, or $f = \frac{1}{2}d$.
Because Cohen's f is the effect size used in power analyses for ANOVA designs it is worth illustrating how Cohen's f is calculated.
If we again take two means of 1 and 2, and a standard deviation of 2, the grand mean is 1.5. 
We subtract each condition mean from the grand mean, square it, calculate the sum of squares, divide it by two, and take the square root. 
$\sigma_m = \sqrt{\frac{(0-0.5)^2+(1-0.5)^2}{2}} = \sqrt{\frac{0.25+0.25}{2}} = 0.5$. 
Dividing this value by the standard deviation yields $f = \frac{0.5}{2} = 0.25.$

G\*Power [@faul_gpower_2007] allows researchers to enter the effect size for the power analysis for ANOVA designs as either Cohen's f or partial eta-squared ($\eta_p^2$).
Partial eta-squared can be converted into Cohen's f: 
\begin{equation}
f = \sqrt{\frac{\eta_p^2}{1-\eta_p^2}} \label{eq:eta-to-f}
\end{equation}
and Cohen's f can be converted into partial eta-squared:
\begin{equation}
\eta_p^2 = \sqrt{\frac{f^2}{f^2+1}} \label{eq:f-to-eta}
\end{equation}
In the example above, $\eta_p^2 = 0.25^2/(0.25^2+1) = 0.0588$. 

Power calculations rely on the noncentrality parameter (lambda, ($\lambda$).) 
In a between-participants one-way ANOVA lambda is calculated as:
\begin{equation}
\lambda = f^2 \times N \label{eq:lambda}
\end{equation}
where f is Cohen's f and N is the total sample size. 
Based on $\lambda$ (which specifies the shape of the expected distribution under the specified alternative hypothesis, e.g., the black curve in Figure \ref{fig:eta-plot}) and the critical test statistic (which specifies the part of the distribution that is more extreme than the test statistic needed for a statistically significant test result) we can calculate how much of the distribution under the alternative hypothesis will be statistically significant in the long run (i.e., the area under the black curve in Figure \ref{fig:eta-plot} to the right of the critical effect size). 

# Simulating Statistical Power for Different Factorial Designs

The code underlying the ANOVApower R package and the Shiny app generates data for each condition in the design and performs an ANOVA and *t*-tests for all comparisons between conditions.
The percentage of significant results (i.e., the power) is calculated based on the desired alpha level, and average effect size estimates are computed for all tests.
Users need to specify the design of the study by providing the number of levels for each factor and indicating whether the factor is measured between or within participants. 
Our initial study above is a '2b' or two level between-participant design.
For ease of interpretation the factors and levels can be named (for our example, factor = "Condition" with levels = "cheerful" and "sad").
The number of observations per condition should be specified (i.e, 80 participants in each between-participant condition).
The means for each condition should be entered (0 and 1), as well as the standard deviation (2). 
For within-participant designs the correlation between variables should be specified.
A document detailing how to reproduce all simulations in this manuscript using ANOVApower of the Shiny app is available in the appendix. 
For a visual confirmation of the input, a figure is created that displays the means and standard deviation (see Figure \ref{fig:mean-plot2}).

```{r mean-plot2, fig.width=7, fig.height=4, echo=FALSE, message=FALSE, warning=FALSE, results='hide', fig.cap="Vizualization for the means and standard deviations for the two conditions. Error bars represent one standard deviation."}

design_result <- ANOVA_design(design = "2b", n = 80, mu = c(1, 0), sd = 2, labelnames = c("condition", "cheerful", "sad"))

set.seed(manuscript_seed)

#To save time compiling this manuscript, simulations are run and the result is stored
# power_result <- ANOVA_power(design_result, alpha_level = 0.05, p_adjust = "none", nsims = nsims, verbose = FALSE)
# saveRDS(power_result, file = "sim_data/power_result.rds")
power_result <- readRDS(file = "sim_data/power_result.rds")

#Analytic solution
power_result_analytic <- power_oneway_between(design_result)$power
power_oneway_between(design_result)$Cohen_f
#Exact simulation solution
power_result_exact <- ANOVA_exact(design_result, verbose = FALSE)$main_results$power
```

There are two ways to calculate the statistical power of an ANOVA through simulations. 
The first is to repeatedly simulate datasets and compute the percentage of statistically significant results.
The second is to simulate a dataset that has *exactly* the desired properties, perform an ANOVA, and use the ANOVA results to compute the statistical power. 
The first approach is a bit more flexible, but the second approach is much faster.
ANOVApower let's users choose either of these simulation approaches. 
Because the true pattern of the data is unknown in an a-priori power analysis, there is often uncertainty about the values that need to be entered in a power analysis. 
It makes sense to examine power across a range of assumptions, from more optimistic scenarios, to more conservative estimates.
It is recommended to power not for the pattern of means you expect, but the smallest effect size that you consider worthwhile to detect (for examples, see @lakens_equivalence_2018). 
The results of a simulation study will vary each time the simulation is performed (but can be made reproducible by specifying a 'seed' number).
To perform the simulations, you specify the number of simulations, the alpha level for the tests, and any adjustments for multiple comparisons that are required. 
The more simulations that are performed, the more stable the results, but the longer the simulation will take.

If 100.000 simulations are performed for our two group between subjects design with means of 1 and 0, a standard deviation of 2, and 80 participants in each group, with a seed set to 2019 (these settings will be used for all simulation results reported in this manuscript), the statistical power (based on the percentage of *p* < $\alpha$ results) is `r power_result$main_results$power`% and the average $\eta_p^2$ is `r power_result$main_results$effect_size`.
The simulation also provides the results for the individual comparisons based on *t*-tests.
Since there are only two groups in this example, the statistical power for the individual comparison is identical to the ANOVA, but the expected effect size, Cohen's d, is `r power_result$pc_results$effect_size`. 

Now that the basic idea behind the simulation is clear, we can use simulations to explore how changes to the experimental design influences power, and answer some of the questions our hypothetical researcher is confronted with when designing a follow-up study.
We will first examine what happens if we add a third, neutral, condition to the design.
Let's assume the mean enjoyment rating for the neutral voice condition falls either perfectly between the cheerful and sad conditions, or is equal to the cheerful condition.
The design now has 3 between-participant conditions, and we can explore what happens if we would collect 80 participants in each condition.

```{r sim-3, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}
#To save time compiling this manuscript, simulations are run and the result is stored
design_result_1 <- ANOVA_design(design = "3b", n = 80, mu = c(1, 0.5, 0), sd = 2, labelnames = c("condition", "cheerful", "neutral", "sad"))
# set.seed(manuscript_seed)
# power_result_1 <- ANOVA_power(design_result_1, alpha_level = 0.05, p_adjust = "none", nsims = nsims, verbose = FALSE)
# saveRDS(power_result_1, file = "sim_data/power_result_1.rds")
power_result_1 <- readRDS(file = "sim_data/power_result_1.rds")

design_result_2 <- ANOVA_design(design = "3b", n = 80, mu = c(1, 1, 0), sd = 2, labelnames = c("condition", "cheerful", "neutral", "sad"))
# set.seed(manuscript_seed)
# power_result_2 <- ANOVA_power(design_result_2, alpha_level = 0.05, p_adjust = "none", nsims = nsims, verbose = FALSE)
# saveRDS(power_result_2, file = "sim_data/power_result_2.rds")
power_result_2 <- readRDS(file = "sim_data/power_result_2.rds")

#analytic solutions
power_oneway_between(design_result_1)$power
power_oneway_between(design_result_1)$Cohen_f
power_oneway_between(design_result_2)$power
power_oneway_between(design_result_2)$Cohen_f

#exact simulation results
ANOVA_exact(design_result_1, verbose = FALSE)$main_results$power
ANOVA_exact(design_result_2, verbose = FALSE)$main_results$power
```

If we assume the mean falls exactly between between the cheerful and sad conditions the simulations show the statistical power for our design is reduced to `r power_result_1$main_results$power`%, and the effect size (partial eta-squared) is `r power_result_1$main_results$effect_size`. 
If we assume the mean the mean is equal to the cheerful condition, the power increases to `r power_result_2$main_results$power`%.
Compared to the two group design (where the power was `r power_result$main_results$power`%), three things have changed. 
First, the numerator degrees of freedom has increased because an additional group is added to the design, which makes the non-central *F*-distribution more similar to the central *F*-distribution, which reduces the statistical power. 
Second, the total sample size is 50% larger after adding 80 participants in the third condition, which increases the statistical power of the ANOVA.
Third, the effect size, Cohen's f, has decreased from `r power_oneway_between(design_result)$Cohen_f` to either `r power_oneway_between(design_result_1)$Cohen_f` or ` r power_oneway_between(design_result_2)$Cohen_f`, which reduces the statistical power.
The exact effect of these three changes on the statistical power is difficult to predict from one ANOVA design to the next.
The most important conclusion based on these simulations is that changing an experimental design can have several opposing effects on the power of a study, depending of the pattern of means. 
One cannot assume the effect size remains unchanged when the design changes, and it might make more sense to think about expected effects in terms of the pattern of means across conditions.

# Power for Within-Subject Designs

What happens if we would perform the second study as a within-participants design?
Instead of collecting three groups of participants, we only collect one group, and let this group evaluate the cheerful, neutral, and sad voice assistants.
A rough but useful approximation of the sample size needed in a within-subject design ($N_W$), relative to the sample needed in between-design ($N_B$), is (from Maxwell & Delaney, 2004, p. 562, formula 47): 
\begin{equation}
N_{W}=\frac{N_{B}(1-\rho)}{a} \label{eq:within-n}
\end{equation}
Here $a$ is the number of within-participant levels, $\rho$ is the correlation between the measurements. 
Even if the correlation is 0, the required sample size for a within design is reduced because each participants contributes multiple measurements.
A positive correlation reduces the standard deviation of the difference scores and increases the statistical power. 
Because the standardized effect size is the mean difference divided by the standard deviation of the difference scores, the correlation has an effect on the standardized mean difference in a within-subject design, referred to as Cohen's $d_z$ (because it is the effect size of the difference score between *x* and *y*, *z*). The relation is:

```{r sim-4, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}
#To save time compiling this manuscript, simulations are run and the result is stored
design_result_within_1 <- ANOVA_design(design = "3w", n = 80, mu = c(1, 0.5, 0), sd = 2, r = 0.5, labelnames = c("condition", "cheerful", "neutral", "sad"))
# set.seed(manuscript_seed)
# power_result_within_1 <- ANOVA_power(design_result_within_1, alpha_level = 0.05, p_adjust = "none", nsims = nsims, verbose = FALSE)
# saveRDS(power_result_within_1, file = "sim_data/power_result_within_1.rds")
power_result_within_1 <- readRDS(file = "sim_data/power_result_within_1.rds")

#analytic solution
power_oneway_within(design_result_within_1)$power

#exact simulation solution
ANOVA_exact(design_result_within_1, verbose = FALSE)$main_results$power
```

\begin{equation}
\sigma_{z}=\sigma\sqrt{2(1-\rho)}
\end{equation}
Cohen's $d_z$ is used in power analyses for dependent *t*-tests, but there is no equivalent Cohen's $f_z$ for a within-participant ANOVA, and Cohen's f is identical for within and between designs. 
Instead, the value for lambda ($\lambda$) is adjusted based on the correlation. 
For a one-way within-participant design lambda is identical to Equation \@ref(eq:lambda), multiplied by *u*, a correction for within-subject designs, calculated as:
\begin{equation}
u = \frac{k}{1-\rho}
\end{equation}
where $k$ is the number of levels of the within-participant factor, and $\rho$ is the correlation between dependent variables.
Equations \@ref(eq:eta-to-f) and \@ref(eq:f-to-eta) no longer hold when measurements are correlated.
The default settings in G\*Power expects an f or $\eta_p^2$ that does *not* incorporate the correlation, while the correlation *is* incorporated in the output of software packages such as SPSS. 
One can  enter the $\eta_p^2$ from SPSS output in G\*Power after checking the 'as in SPSS' checkbox in the options window, but forgetting this is a common mistake in power analyses for within designs in G\*Power.
For a one-way within-subject design, Cohen's f can be converted into the Cohen's f SPSS uses through:
\begin{equation}
f^2_{SPSS} = f^2 \times \frac{k}{k-1} \times \frac{n}{n-1} \times \frac{1}{1-\rho}
\end{equation}
and subsequently tranformed to $\eta_p^2$ through Equation \@ref(eq:f-to-eta).

Revisting our between-participant design, power was `r power_result_1$main_results$power`% when the enjoyment scores were uncorrelated.
If we want to examine the power for a within design we need to enter a reasonable value for the correlation between dependent variables. 
Ideally this value is determined based on previous studies, and it often makes sense to explore a range of possible correlations in an a-priori power analysis.
Let's assume our best estimate of the correlation between enjoyment ratings in a within-subject design is *r* = 0.5.
The power for a repeated-measures ANOVA based on these values, where ratings for the three conditions are collected from 80 participants, is `r power_result_within_1$main_results$power`%.
Because of the positive correlation between dependent variables, the effect size $\eta_p^2$ is much larger for the within-subject design ($\eta_p^2$ = `r power_result_within_1$main_results$effect_size`) than for the 3 group between participants design ($\eta_p^2$ = `r power_result_1$main_results$effect_size`). 
Note that the ANOVApower package allow researchers to enter a correlation matrix that specifies the expected correlations between each individual pair of measurements, instead of assuming the correlations between all dependent variables are identical.

# Power for Interactions

The effect size for interactions in ANOVA designs depends on the pattern of means.
Let's assume the researcher plans to perform a follow-up experiment where in addition to making the voice sound cheerful or sad, a second factor is introduced by making the voice sound more robotic compared to the default human-like voice. 
Different patterns of results could be expected.
Either the same effect is observed for robotic voices, or no effect is observed for robotic voices, or the opposite effect is observed for robotic voices (we enjoy a sad robotic voice more than a cheerful one, a 'Marvin-the-Depressed-Robot Effect'). 
In the first case, we will only observe a main effect of voice, but in the other two scenarios there is an interaction effect between human-likeness of the voice and the emotional tone of the voice. 
We can simulate a cross-over interaction for a 2x2 between-participant design with 80 participants in each group to examine the statistical power (see Figure \ref{fig:mean-plot} for the expected pattern of means). 

```{r mean-plot, fig.width=7, fig.height=4, echo=FALSE, message=FALSE, warning=FALSE, results='hide', fig.cap="Vizualization for the expected means and standard deviations for a crossover interaction. Error bars represent one standard deviation."}

#To save time compiling this manuscript, simulations are run and the result is stored
design_result_cross_80 <- ANOVA_design(design = "2b*2b", n = 80, mu = c(1, 0, 0, 1), sd = 2, labelnames = c("condition", "cheerful", "sad", "voice", "human", "robot"))
# set.seed(manuscript_seed)
# power_result_cross_80 <- ANOVA_power(design_result_cross_80, alpha_level = 0.05, p_adjust = "none", nsims = nsims, verbose = FALSE)
# saveRDS(power_result_cross_80, file = "sim_data/power_result_cross_80.rds")
power_result_cross_80 <- readRDS(file = "sim_data/power_result_cross_80.rds")


#To save time compiling this manuscript, simulations are run and the result is stored
design_result_cross_40 <- ANOVA_design(design = "2b*2b", n = 40, mu = c(1, 0, 0, 1), sd = 2, labelnames = c("condition", "cheerful", "sad", "voice", "human", "robot"), plot = FALSE)
# set.seed(manuscript_seed)
# power_result_cross_40 <- ANOVA_power(design_result_cross_40, alpha_level = 0.05, p_adjust = "none", nsims = nsims, verbose = FALSE)
# saveRDS(power_result_cross_40, file = "sim_data/power_result_cross_40.rds")
power_result_cross_40 <- readRDS(file = "sim_data/power_result_cross_40.rds")

#Analytic solution
power_twoway_between(design_result_cross_40)$power_A
power_twoway_between(design_result_cross_40)$power_B
power_twoway_between(design_result_cross_40)$power_AB
power_twoway_between(design_result_cross_80)$power_A
power_twoway_between(design_result_cross_80)$power_B
power_twoway_between(design_result_cross_80)$power_AB

#exact simulation solution
ANOVA_exact(design_result_cross_40, verbose = FALSE)$main_results$power
ANOVA_exact(design_result_cross_80, verbose = FALSE)$main_results$power
```

Mathematically the interaction effect is computed as the cell mean minus the sum of the grand mean, the marginal mean in each row minus the grand mean, and the marginal mean in each column minus grand mean (see @maxwell_designing_2017). For example, for the cheerful human-like voice condition this is 1 (the value in the cell) - (0.5 [the grand mean] + 0.5 [the cell mean minus the marginal mean in row 1] + 0.5 [the cell mean minus the marginal mean in column 2]). 
Thus, 1 - (0.5 + 0.5 + 0.5) = -0.5.
Completing this for all four cells gives the values -0.5, 0.5, 0.5, -0.5.
Cohen's f is then $f = \frac { \sqrt { \frac { -0.5^2 + 0.5^2 + 0.5 + -0.5^2 } { 4 } }}{ 2 } = 0.25$.
Simulations show we have `r power_result_cross_80$main_results$power[3]`% power when we collect 80 participants per condition.
Power is high, because we collected 80 participants in each condition. 
Compared to the two-group comparison with 80 participants per group a cross-over (also called 'disordinal') interaction with two levels per factor has the same power as the initial two-group design if we halve the sample size per condition (i.e., n = 40 in each cell).
Power with 40 participants per condition is `r power_result_cross_40$main_results$power[3]`%.
Main effects in an ANOVA are based on the means for one factor averaged over the other factors (e.g., the main effect of a human-like versus robot-like voice, irrespective of whether it is cheerful or sad). 
The interaction effect can be contrast coded as 1, -1, -1, 1, and thus tests the scores of 80 participants against 80 other participants, just as for the *t*-test or the main effects.
The key insight here is that not the sample size per condition, but the total sample size over all other factors determines the power for the main effects and the interaction [cf. @westfall_think_2015].

```{r sim-interaction-2, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}
#To save time compiling this manuscript, simulations are run and the result is stored
design_result_ordinal <- ANOVA_design(design = "2b*2b", n = 160, mu = c(1, 0, 0, 0), sd = 2, labelnames = c("condition", "cheerful", "sad", "voice", "human", "robot"))
# set.seed(manuscript_seed)
# power_result_ordinal <- ANOVA_power(design_result_ordinal, alpha_level = 0.05, p_adjust = "none", nsims = nsims)
# saveRDS(power_result_ordinal, file = "sim_data/power_result_ordinal.rds")
power_result_ordinal <- readRDS(file = "sim_data/power_result_ordinal.rds")

#Analytic solution
power_twoway_between(design_result_ordinal)$power_A
power_twoway_between(design_result_ordinal)$power_B
power_twoway_between(design_result_ordinal)$power_AB

#exact simulation solution
ANOVA_exact(design_result_ordinal, verbose = FALSE)$main_results$power

```

We can also examine the statistical power for a pattern of results that indicated that there was no difference in interacting with a cheerful of sad conversational agent with a robot voice. 
In this case, we expect an 'ordinal' interaction (the means for the human-like voice are never lower than the means for the robot-like voice, and thus there is no cross-over effect). 
The expected pattern of means is 1, 0, 0, 0, with only a single mean that differs from the others.
As has been pointed out [@giner-sorolla_powering_2018; @simonsohn_no-way_2014] these designs require larger samples sizes to have the same power to detect the interaction, compared to the two-group comparison.
The reason for this is that the effect size is only half as large, with Cohen's f = `r format(power_twoway_between(design_result_ordinal)$Cohen_f_AB, digits =3)` (compared to `r power_twoway_between(design_result_cross_80)$Cohen_f_AB` in the cross-over interaction).
 
By steadily increasing the sample size in the simulation, we can examine the required sample size to get the same power as for the cross-over interaction with 40 participants per cell (`r power_result_cross_40$main_results$power[3]`% power). A total sample size of 635 is required, almost four times as large as the total sample size for the two-group comparison (160). To make it easier to find the sample size per group that is required to calculate the expected power the plot_power function in ANOVApower can be used to plot the power for a specific design across a range of sample sizes (see Figure \ref{fig:p-plot}).

```{r power-plot, fig.width=7, fig.height=4, echo=FALSE, message=FALSE, warning=FALSE, results='hide', fig.cap="Power curves across a range of sample sizes per group from n = 10 to n = 200 for the expected main effects and ordinal interaction."}

#Taking advantage of the power_df that is stored to plot only 1 of the power curves.
plot_data <- plot_power(design_result_ordinal, min_n = 10, max_n = 200, plot = FALSE)$power_df


ggplot(data=plot_data, aes(x = n, y = voice)) +
  geom_line( size=1.5) +
  scale_x_continuous(limits = c(10, 200)) +
  scale_y_continuous(limits = c(0, 100), breaks = seq(0,100,10)) +
  theme_bw() +
  labs(x="Sample size per condition", y = "Power")


```

Where a two group comparison has a Cohen's f of 0.25 when the means are 1 and 0, a 2x2 ordinal interaction has a Cohen's f of 0.25 when the pattern of means is 2, 0, 0, 0, and a 2x2x2 interaction where only one cell differs from the other means has a Cohen's f of 0.25 when the pattern of means is 4, 0, 0, 0, 0, 0, 0, 0 across the eight cells. 
The take-home message is that a 'medium' effect size translates into a much more extreme pattern of means in an ordinal interaction than in a disordinal (crossover) interaction, or in a 2x2x2 interaction compared to a 2x2 interaction (see also @perugini_practical_2018).
It might therefore be more intuitive to perform a power analysis based on the expected pattern of means, and compute Cohen's f based on this pattern, than to specify an effect size for the power analysis directly.

# Power for Individual Comparisons

Although an initial goal when performing an ANOVA might be to test the *omnibus null hypothesis*, which answers the question whether there are *any* differences between group means, we often want to know which conditions differ from each other. Thus, an ANOVA is often followed up by individual comparisons (whether *planned* or *post-hoc*).
One feature of ANOVApower is that it provides the statistical power for all individual comparisons that can be performed.
The power and effect size estimates are based on simple *t*-tests. 
Taking into account the variance estimates from other groups (which the *t*-tests do not do) can have power benefits [@maxwell_designing_2017], but these depend on whether the homogeneity assumption is met, which is often not warranted in psychological research, and violations of the homogeneity assumption impact Type 1 error rates [@delacre_why_2018].    
Furthermore, performing individual *t*-tests as follow-up tests allow researchers to specify directional predictions which increase the power of individual comparisons.

Power analysis for individual comparisons is relatively straightforward and can easily be done in all power analysis software, but providing power for all individual comparisons alongside the ANOVA hopefully nudges researchers to take into account the power for follow-up tests.
Sometimes the interaction in the ANOVA will have more power than the individual comparisons (i.e., in the case of a cross-over interaction) and sometimes the power for the interaction will be lower than the power for independent comparisons.
It is always important to check if you have adequate power for all tests you plan to perform, which includes main effects, interactions, and individual comparisons performed as follow-up tests. 

# Type 1 Error Control in Exploratory ANOVA's

In a 2x2x2 design, an ANOVA will give the test results for three main effects, three two-way interactions, and one three-way interaction. 
Because seven statistical tests are performed, the probability of making at least one Type 1 error in a single exploratory 2x2x2 ANOVA is $1-(0.95)^7$ = 30%.
It is therefore important to control error rates when performing multiple comparisons [@cramer_hidden_2016].

It is possible to control the overall Type I error rate by specifying an adjustment for multiple comparisons in the simulation. 
By adjusting for multiple comparisons we ensure that we do not conclude there is an effect in *any* of the individual tests more often than the desired Type I error rate.
Several techniques to control error rates exist, of which the best known is the Bonferroni adjustment.
The Holm procedure is slightly more powerful than the Bonferroni adjustment, without requiring additional assumptions [for other approaches, see @bretz_multiple_2011].
Because the adjustment for multiple comparisons lowers the alpha level, it also lowers the statistical power.
When designing a study where you will perform multiple comparisons, it is important to take into account the corrected alpha level when determining the required sample size, both for the ANOVA tests, as for the individual comparisons.

```{r sim-holm, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}
#To save time compiling this manuscript, simulations are run and the result is stored
design_result_holm_correction <- ANOVA_design(design = "2b*2b", n = 40, mu = c(1, 0, 0, 1), sd = 2, labelnames = c("condition", "cheerful", "sad", "voice", "human", "robot"))
# set.seed(manuscript_seed)
# power_result_holm_correction <- ANOVA_power(design_result_holm_correction, alpha_level = 0.05, p_adjust = "holm", nsims = nsims, verbose = FALSE)
# saveRDS(sim_data/power_result_holm_correction, file = "power_result_holm_correction.rds")
power_result_holm_correction <- readRDS(file = "sim_data/power_result_holm_correction.rds")

```

If we revisit the 2x2 between-condition ANOVA where a cross-over interaction was predicted and 40 participants per condition were collected, and apply a Holm correction for multiple comparisons, we see power for the interaction is reduced from `r power_result_cross_40$main_results$power[3]`% without applying a correction, to `r power_result_holm_correction$main_results$power[3]`% after correcting for multiple comparisons.
There are three tests for a 2x2 design (two main effects, one interaction), but 6 individual comparisons between the four conditions, and thus the correction for multiple comparisons has a stronger effect on the individual comparisons.
For example, the power for the *t*-test comparing two between-participant conditions falls from `r power_result_cross_40$pc_results$power[1]`% without correcting for multiple comparisons, to `r power_result_holm_correction$pc_results$power[1]`% when adjusting the alpha level for the 6 possible individual comparisons between the four conditions.

These simulations should reveal the cost (in terms of the statistical power) of exploring across all possible comparisons while controlling error rates.
It is often advisable to preregister specific tests that are of interest to a researcher.
If one is really interested in all possible comparisons, but wants to control the probability of incorrectly acting as if there is an effect, the reduction in power due to the lower alpha level should be counteracted by increasing the sample size to maintain an adequate level of power for both the ANOVA as the individual comparisons. 

# P-value distributions

Statistical power is the long run probability of observing a *p*-value smaller than the alpha level.
One intuitive way to illustrate this concept is to plot the distribution of *p*-values for all simulations.
The simulation code automatically stores plots for *p*-value distributions for each test in the simulation.
In Figure \ref{fig:p-plot} we see that for the initial 2 group between-participant design most *p*-values fall below the alpha level (in the figure, the vertical line as 0.05).
For the `r nsims` simulations, `r power_result$main_results$power`% fall below the alpha level.
As power is increased the p-value distribution becomes steeper, and when there is an effect size of zero *p*-values are uniformly distributed. 

```{r p-plot, fig.width=7, fig.height=4, echo=FALSE, message=FALSE, warning=FALSE, results='hide', fig.cap="Distribution of p-values for the initial two-group between subjects design."}
power_result$plot1
```

# Conclusion

It is important to carefully justify the sample size when designing informative studies.
Simulation based approaches can help to provide insights into the factors that determine the statistical power for factorial ANOVA designs.
Exploring the power for designs with specific patterns of means, standard deviations, and correlations between variables can be used to choose a design and sample size that provides the highest statistical power for future studies. The R package (https://github.com/Lakens/ANOVApower) and Shiny app (http://shiny.ieis.tue.nl/anova_power/) that accompany this paper enable researchers to perform simulations for factorial experiments of up to three factors and any number of levels, making it easy to perform simulation-based power analysis without extensive programming experience.

## Author Contributions

D. Lakens and A. R. Caldwell collaboratively developed the ANOVApower R package. D. Lakens wrote the initial draft, and both authors revised the manuscript. A. R. Caldwell created the Shiny app. 

## ORCID iD's

DaniÃ«l Lakens ![](screenshots/orcid.png) https://orcid.org/0000-0002-0247-239X
Aaron R. Caldwell ![](screenshots/orcid.png) https://orcid.org/0000-0002-4541-6283

## Acknowledgements

Many improvements to ANOVApower are based on feedback from Lisa DeBruine and the sim_design function in her 'faux' R package. The ANOVA_exact function was inspired by Chris Aberson. We are grateful to Jonathon Love for the development of a jamovi module based on ANOVApower.

## Declaration of Conflicting Interests
The author(s) declared that there were no conflicts of interest with respect to the authorship or the publication of this article.

## Funding
This work was funded by VIDI Grant 452-17-013 from the Netherlands Organisation for Scientific Research.

## Open Practices
The code to reproduce the analyses reported in this article has been made publicly available via the Open Science Framework and can be accessed at https://osf.io/pn8mc/. 

# References

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
