
@book{maxwell_designing_2017,
  address = {New York, NY},
  edition = {3 edition},
  title = {Designing {{Experiments}} and {{Analyzing Data}}: {{A Model Comparison Perspective}}, {{Third Edition}}},
  isbn = {978-1-138-89228-6},
  shorttitle = {Designing {{Experiments}} and {{Analyzing Data}}},
  abstract = {Designing Experiments and Analyzing Data: A Model Comparison Perspective (3rd edition) offers an integrative conceptual framework for understanding experimental design and data analysis. Maxwell, Delaney, and Kelley first apply fundamental principles to simple experimental designs followed by an application of the same principles to more complicated designs. Their integrative conceptual framework better prepares readers to understand the logic behind a general strategy of data analysis that is appropriate for a wide variety of designs, which allows for the introduction of more complex topics that are generally omitted from other books. Numerous pedagogical features further facilitate understanding: examples of published research demonstrate the applicability of each chapter's content; flowcharts assist in choosing the most appropriate procedure; end-of-chapter lists of important formulas highlight key ideas and assist readers in locating the initial presentation of equations; useful programming code and tips are provided throughout the book and in associated resources available online, and extensive sets of exercises help develop a deeper understanding of the subject. Detailed solutions for some of the exercises and realistic data sets are included on the website (DesigningExperiments.com). The pedagogical approach used throughout the book enables readers to gain an overview of experimental design, from conceptualization of the research question to analysis of the data. The book and its companion website with web apps, tutorials, and detailed code are ideal for students and researchers seeking the optimal way to design their studies and analyze the resulting data.},
  language = {English},
  publisher = {{Routledge}},
  author = {Maxwell, Scott E. and Delaney, Harold D. and Kelley, Ken},
  month = aug,
  year = {2017},
  note = {00000}
}

@article{lakens_calculating_2013,
  title = {Calculating and Reporting Effect Sizes to Facilitate Cumulative Science: A Practical Primer for t-Tests and {{ANOVAs}}},
  volume = {4},
  issn = {1664-1078},
  shorttitle = {Calculating and Reporting Effect Sizes to Facilitate Cumulative Science},
  journal = {Frontiers in Psychology},
  doi = {10.3389/fpsyg.2013.00863},
  author = {Lakens, Dani\"el},
  year = {2013},
  keywords = {effect sizes,cohen's d,sample size planning,Eta-squared,Power analysis},
  note = {00000}
}

@article{schutz_analysis_1987,
  title = {The {{Analysis}} of {{Repeated Measures Designs Involving Multiple Dependent Variables}}},
  volume = {58},
  issn = {0270-1367, 2168-3824},
  abstract = {Experimental designs involving repeated measures may be analyzed using traditional ANOVA methods (if all assumptions are met), or, given sufficientsample size, with a MANOVA-type procedure (if only some assumptions are met). Recent statistical advances have made possible the extension of these procedures to repeated measures experimental designs involving multiple dependent variables. This tutorialpresents the concepts, complete examples (including computer control commands), and interpretations for four methods of testingdifferences among means in a mixed modelrepeated measures design. The four methods of analysis are: The traditional ANOVA and the MANOVA method for the single dependent variable case, and a Multivariate Mixed Model analysis and a Doubly Multivariate analysis for the multipledependent variable case.},
  language = {en},
  number = {2},
  journal = {Research Quarterly for Exercise and Sport},
  doi = {10.1080/02701367.1987.10605437},
  author = {Schutz, Robert W. and Gessaroli, Marc E.},
  month = jun,
  year = {1987},
  pages = {132-149},
  note = {00000}
}

@article{faul_gpower_2007,
  title = {{{GPower}} 3: {{A}} Flexible Statistical Power Analysis Program for the Social, Behavioral, and Biomedical Sciences},
  volume = {39},
  issn = {1554-351X, 1554-3528},
  shorttitle = {G*{{Power}} 3},
  language = {en},
  number = {2},
  journal = {Behavior Research Methods},
  doi = {10.3758/BF03193146},
  author = {Faul, Franz and Erdfelder, Edgar and Lang, Albert-Georg and Buchner, Axel},
  month = may,
  year = {2007},
  pages = {175-191},
  note = {00000}
}

@article{potvin_statistical_2000,
  title = {Statistical Power for the Two-Factor Repeated Measures {{ANOVA}}},
  volume = {32},
  issn = {1532-5970},
  abstract = {Determining a priori power for univariate repeated measures (RM) ANOVA designs with two or more within-subjects factors that have different correlational patterns between the factors is currently difficult due to the unavailability of accurate methods to estimate the error variances used in power calculations. The main objective of this study was to determine the effect of the correlation between the levels in one RM factor on the power of the other RM factor. Monte Carlo simulation procedures were used to estimate power for the A, B, and AB tests of a 2\texttimes{}3, a 2\texttimes{}6, a 2\texttimes{}9, a 3\texttimes{}3, a 3\texttimes{}6, and a 3\texttimes{}9 design under varying experimental conditions of effect size (small, medium, and large), average correlation (.4 and .8), alpha (.01 and .05), and sample size (n = 5, 10 ,15, 20, 25, and 30). Results indicated that the greater the magnitude of the differences between the average correlation among the levels of Factor A and the average correlation in the AB matrix, the lower the power for Factor B (and vice versa). Equations for estimating the error variance of each test of the two-way model were constructed by examining power and mean square error trends across different correlation matrices. Support for the accuracy of these formulae is given, thus allowing for direct analytic power calculations in future studies.},
  language = {en},
  number = {2},
  journal = {Behavior Research Methods, Instruments, \& Computers},
  doi = {10/fqbpn7},
  author = {Potvin, Patrick J. and Schutz, Robert W.},
  month = jun,
  year = {2000},
  keywords = {Average Correlation,Error Variance,Monte Carlo,Repeated Measure Design,Repeated Measure Factor},
  pages = {347-356},
  note = {00000}
}

@book{cohen_statistical_1988,
  address = {Hillsdale, N.J},
  edition = {2nd ed},
  title = {Statistical Power Analysis for the Behavioral Sciences},
  isbn = {978-0-8058-0283-2},
  lccn = {HA29 .C66 1988},
  publisher = {{L. Erlbaum Associates}},
  author = {Cohen, Jacob},
  year = {1988},
  keywords = {Statistical methods,Social sciences,Probabilities,Statistical power analysis},
  note = {00000}
}

@book{aberson_applied_2019,
  address = {New York},
  edition = {2 edition},
  title = {Applied {{Power Analysis}} for the {{Behavioral Sciences}}: 2nd {{Edition}}},
  isbn = {978-1-138-04456-2},
  shorttitle = {Applied {{Power Analysis}} for the {{Behavioral Sciences}}},
  abstract = {Applied Power Analysis for the Behavioral Sciences is a practical "how-to" guide to conducting statistical power analyses for psychology and related fields. The book provides a guide to conducting analyses that is appropriate for researchers and students, including those with limited quantitative backgrounds. With practical use in mind, the text provides detailed coverage of topics such as how to estimate expected effect sizes and power analyses for complex designs. The topical coverage of the text, an applied approach, in-depth coverage of popular statistical procedures, and a focus on conducting analyses using R make the text a unique contribution to the power literature. To facilitate application and usability, the text includes ready-to-use R code developed for the text. An accompanying R package called pwr2ppl (available at https://github.com/chrisaberson/pwr2ppl) provides tools for conducting power analyses across each topic covered in the text.},
  language = {English},
  publisher = {{Routledge}},
  author = {Aberson, Christopher L.},
  month = feb,
  year = {2019},
  note = {00000}
}

@misc{giner-sorolla_powering_2018,
  title = {Powering {{Your Interaction}}},
  abstract = {With all the manuscripts I see, as editor-in-chief of Journal of Experimental Social Psychology, it's clear that authors are following a wide variety of standards for statistical power analys\ldots{}},
  language = {en},
  journal = {Approaching Significance},
  author = {{Giner-Sorolla}, Roger},
  month = jan,
  year = {2018}
}

@misc{simonsohn_no-way_2014,
  title = {No-Way {{Interactions}}},
  abstract = {This post shares a shocking and counterintuitive fact about studies looking at interactions where effects are predicted to get smaller (attenuated interactions). I needed a working example and went with Fritz Strack et al.'s~ (1988, .pdf) famous paper [933 Google cites], in which participants rated cartoons as funnier if they saw them while holding a...},
  language = {en-US},
  journal = {Data Colada},
  author = {Simonsohn, Uri},
  month = mar,
  year = {2014},
  note = {00000}
}

@book{maxwell_designing_2004,
  address = {Mahwah, N.J},
  edition = {2nd ed},
  title = {Designing Experiments and Analyzing Data: A Model Comparison Perspective},
  isbn = {978-0-8058-3718-6},
  lccn = {QA279 .M384 2004},
  shorttitle = {Designing Experiments and Analyzing Data},
  publisher = {{Lawrence Erlbaum Associates}},
  author = {Maxwell, Scott E. and Delaney, Harold D.},
  year = {2004},
  keywords = {Experimental design}
}

@misc{westfall_think_2015,
  title = {Think about Total {{N}}, Not n per Cell},
  abstract = {The bottom line of this post is simple. There are lots of rules of thumb out there for minimum sample sizes to use in between-subjects factorial experiments. But they are virtually always formulate\ldots{}},
  language = {en-US},
  journal = {Cookie Scientist},
  author = {Westfall, Jacob},
  month = may,
  year = {2015}
}

@article{lubin_interpretation_1961,
  title = {The {{Interpretation}} of {{Significant Interaction}}},
  volume = {21},
  issn = {0013-1644},
  language = {en},
  number = {4},
  journal = {Educational and Psychological Measurement},
  doi = {10.1177/001316446102100406},
  author = {Lubin, Ardie},
  month = jan,
  year = {1961},
  pages = {807-817}
}

@article{perugini_practical_2018,
  title = {A {{Practical Primer To Power Analysis}} for {{Simple Experimental Designs}}},
  volume = {31},
  copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are \textcopyright, \textregistered{} or \texttrademark{} of their respective owners. No challenge to any owner's rights is intended or should be inferred.},
  issn = {2397-8570},
  abstract = {Power analysis is an important tool to use when planning studies. This contribution aims to remind readers what power analysis is, emphasize why it matters, and articulate when and how it should be used. The focus is on applications of power analysis for experimental designs often encountered in psychology, starting from simple two-group independent and paired groups and moving to one-way analysis of variance, factorial designs, contrast analysis, trend analysis, regression analysis, analysis of covariance, and mediation analysis. Special attention is given to the application of power analysis to moderation designs, considering both dichotomous and continuous predictors and moderators. Illustrative practical examples based on G*Power and R packages are provided throughout the article. Annotated code for the examples with R and dedicated computational tools are made freely available at a dedicated web page (https://github.com/mcfanda/primerPowerIRSP). Applications of power analysis for more complex designs are briefly mentioned, and some important general issues related to power analysis are discussed.},
  language = {eng},
  number = {1},
  journal = {International Review of Social Psychology},
  doi = {10.5334/irsp.181},
  author = {Perugini, Marco and Gallucci, Marcello and Costantini, Giulio},
  month = jul,
  year = {2018},
  keywords = {power analysis,effect size,uncertainty,moderation,sensitivity analysis},
  pages = {20}
}

@article{lakens_justify_2018,
  title = {Justify Your Alpha},
  volume = {2},
  copyright = {2018 The Publisher},
  issn = {2397-3374},
  abstract = {In response to recommendations to redefine statistical significance to P {$\leq$} 0.005, we propose that researchers should transparently report and justify all choices they make when designing a study, including the alpha level.},
  language = {en},
  journal = {Nature Human Behaviour},
  doi = {10.1038/s41562-018-0311-x},
  author = {Lakens, Dani\"el and Adolfi, Federico G. and Albers, Casper J. and Anvari, Farid and Apps, Matthew A. J. and Argamon, Shlomo E. and Baguley, Thom and Becker, Raymond B. and Benning, Stephen D. and Bradford, Daniel E. and Buchanan, Erin M. and Caldwell, Aaron R. and Calster, Ben and Carlsson, Rickard and Chen, Sau-Chin and Chung, Bryan and Colling, Lincoln J. and Collins, Gary S. and Crook, Zander and Cross, Emily S. and Daniels, Sameera and Danielsson, Henrik and DeBruine, Lisa and Dunleavy, Daniel J. and Earp, Brian D. and Feist, Michele I. and Ferrell, Jason D. and Field, James G. and Fox, Nicholas W. and Friesen, Amanda and Gomes, Caio and {Gonzalez-Marquez}, Monica and Grange, James A. and Grieve, Andrew P. and Guggenberger, Robert and Grist, James and Harmelen, Anne-Laura and Hasselman, Fred and Hochard, Kevin D. and Hoffarth, Mark R. and Holmes, Nicholas P. and Ingre, Michael and Isager, Peder M. and Isotalus, Hanna K. and Johansson, Christer and Juszczyk, Konrad and Kenny, David A. and Khalil, Ahmed A. and Konat, Barbara and Lao, Junpeng and Larsen, Erik Gahner and Lodder, Gerine M. A. and Lukavsk\'y, Ji{\v r}\'i and Madan, Christopher R. and Manheim, David and Martin, Stephen R. and Martin, Andrea E. and Mayo, Deborah G. and McCarthy, Randy J. and McConway, Kevin and McFarland, Colin and Nio, Amanda Q. X. and Nilsonne, Gustav and Oliveira, Cilene Lino and Xivry, Jean-Jacques Orban and Parsons, Sam and Pfuhl, Gerit and Quinn, Kimberly A. and Sakon, John J. and Saribay, S. Adil and Schneider, Iris K. and Selvaraju, Manojkumar and Sjoerds, Zsuzsika and Smith, Samuel G. and Smits, Tim and Spies, Jeffrey R. and Sreekumar, Vishnu and Steltenpohl, Crystal N. and Stenhouse, Neil and \'Swi\k{a}tkowski, Wojciech and Vadillo, Miguel A. and Assen, Marcel A. L. M. and Williams, Matt N. and Williams, Samantha E. and Williams, Donald R. and Yarkoni, Tal and Ziano, Ignazio and Zwaan, Rolf A.},
  month = feb,
  year = {2018},
  pages = {168-171},
  note = {00010}
}

@article{albers_when_2018,
  title = {When Power Analyses Based on Pilot Data Are Biased: {{Inaccurate}} Effect Size Estimators and Follow-up Bias},
  volume = {74},
  issn = {0022-1031},
  shorttitle = {When Power Analyses Based on Pilot Data Are Biased},
  abstract = {When designing a study, the planned sample size is often based on power analyses. One way to choose an effect size for power analyses is by relying on pilot data. A-priori power analyses are only accurate when the effect size estimate is accurate. In this paper we highlight two sources of bias when performing a-priori power analyses for between-subject designs based on pilot data. First, we examine how the choice of the effect size index ({$\eta$}2, {$\omega$}2 and {$\epsilon$}2) affects the sample size and power of the main study. Based on our observations, we recommend against the use of {$\eta$}2 in a-priori power analyses. Second, we examine how the maximum sample size researchers are willing to collect in a main study (e.g. due to time or financial constraints) leads to overestimated effect size estimates in the studies that are performed. Determining the required sample size exclusively based on the effect size estimates from pilot data, and following up on pilot studies only when the sample size estimate for the main study is considered feasible, creates what we term follow-up bias. We explain how follow-up bias leads to underpowered main studies.
Our simulations show that designing main studies based on effect sizes estimated from small pilot studies does not yield desired levels of power due to accuracy bias and follow-up bias, even when publication bias is not an issue. We urge researchers to consider alternative approaches to determining the sample size of their studies, and discuss several options.},
  journal = {Journal of Experimental Social Psychology},
  doi = {10.1016/j.jesp.2017.09.004},
  author = {Albers, Casper and Lakens, Dani\"el},
  month = jan,
  year = {2018},
  keywords = {Effect size,Epsilon-squared,Eta-squared,Follow-up bias,Omega-squared,Power analysis},
  pages = {187-195}
}

@article{maxwell_sample_2008,
  title = {Sample {{Size Planning}} for {{Statistical Power}} and {{Accuracy}} in {{Parameter Estimation}}},
  volume = {59},
  issn = {0066-4308, 1545-2085},
  language = {en},
  number = {1},
  journal = {Annual Review of Psychology},
  doi = {10.1146/annurev.psych.59.103006.093735},
  author = {Maxwell, Scott E. and Kelley, Ken and Rausch, Joseph R.},
  month = jan,
  year = {2008},
  pages = {537-563}
}

@article{delacre_why_2017,
  title = {Why {{Psychologists Should}} by {{Default Use Welch}}'s {\emph{t}}-Test {{Instead}} of {{Student}}'s {\emph{t}}-Test},
  volume = {30},
  copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are \textcopyright, \textregistered{} or \texttrademark{} of their respective owners. No challenge to any owner's rights is intended or should be inferred.},
  issn = {2119-4130},
  abstract = {When comparing two independent groups, psychology researchers commonly use Student's t-tests. Assumptions of normality and homogeneity of variance underlie this test. More often than not, when these conditions are not met, Student's t-test can be severely biased and lead to invalid statistical inferences. Moreover, we argue that the assumption of equal variances will seldom hold in psychological research, and choosing between Student's t-test and Welch's t-test based on the outcomes of a test of the equality of variances often fails to provide an appropriate answer. We show that the Welch's t-test provides a better control of Type 1 error rates when the assumption of homogeneity of variance is not met, and it loses little robustness compared to Student's t-test when the assumptions are met. We argue that Welch's t-test should be used as a default strategy.},
  language = {eng},
  number = {1},
  journal = {International Review of Social Psychology},
  doi = {10.5334/irsp.82},
  author = {Delacre, Marie and Lakens, Dani\"el and Leys, Christophe},
  month = apr,
  year = {2017},
  keywords = {statistical power,Welch’s t-test,Student’s t-test,homogeneity of variance,Levene’s test,Homoscedasticity,type 1 error,type 2 error}
}

@article{delacre_why_2018,
  title = {Why {{Psychologists Should Always Report}} the {{W}}-Test {{Instead}} of the {{F}}-{{Test ANOVA}}},
  abstract = {When comparing independent groups, researchers in psychology commonly use Analysis of Variance (ANOVA), which assumes data is normally distributed, and variances are equal across conditions. When these assumptions are not met, the classical ANOVA (F-test) can be severely biased, which leads to invalid statistical inferences. However, despite their importance, test assumptions are rarely explicitly considered in scientific articles. We discuss why the assumptions of normality and homogeneity of variances will often not hold in psychological research. We explain when and why this is problematic, especially for the assumption of homogeneity of variances. Our simulations show that Welch's ANOVA (W-test) controls the Type 1 error rate better than the F-test when the assumption of homogeneity of variance is not met, and loses little robustness compared to the F-test when the assumptions are met. Because assumption tests for the equality of variances often fail to provide an informative answer, we argue that the W-test should be the default choice in psychology.},
  journal = {PsyArXiv},
  doi = {10.17605/OSF.IO/WNEZG},
  author = {Delacre, Marie and Lakens, Dani\"el and Mora, Youri and Leys, Christophe},
  month = jan,
  year = {2018}
}

@article{brysbaert_how_2019,
  title = {How Many Participants Do We Have to Include in Properly Powered Experiments? {{A}} Tutorial of Power Analysis with Some Simple Guidelines.},
  journal = {Journal of Cognition},
  author = {Brysbaert, Marc},
  year = {2019}
}

@article{cramer_hidden_2014,
  title = {Hidden Multiplicity in Multiway {{ANOVA}}: {{Prevalence}}, Consequences, and Remedies},
  shorttitle = {Hidden Multiplicity in Multiway {{ANOVA}}},
  journal = {arXiv preprint arXiv:1412.3416},
  author = {Cramer, Ang\'elique OJ and {van Ravenzwaaij}, Don and Matzke, Dora and Steingroever, Helen and Wetzels, Ruud and Grasman, Raoul PPP and Waldorp, Lourens J. and Wagenmakers, Eric-Jan},
  year = {2014}
}

@book{bretz_multiple_2011,
  address = {Boca Raton, FL},
  title = {Multiple Comparisons Using {{R}}},
  isbn = {978-1-58488-574-0},
  lccn = {QA278.4 .B74 2011},
  abstract = {"Adopting a unifying theme based on maximum statistics, Multiple Comparisons Using R describes the common underlying theory of multiple comparison procedures through numerous examples. After giving examples of multiplicity problems, the book covers general concepts and basic multiple comparisons procedures, including the Bonferroni method and Simes' test. It then shows how to perform parametric multiple comparisons in standard linear models and general parametric models. It also introduces the multcomp package in R, which offers a convenient interface to perform multiple comparisons in a general context. Following this theoretical framework, the book explores applications involving the Dunnett test, Tukey's all pairwise comparisons, and general multiple contrast tests for standard regression models, mixed-effects models, and parametric survival models. The last chapter reviews other multiple comparison procedures, such as resampling-based procedures, methods for group sequential or adaptive designs, and the combination of multiple comparison procedures with modeling techniques. Controlling multiplicity in experiments ensures better decision making and safeguards against false claims. A self-contained introduction to multiple comparison procedures, this book offers strategies for constructing the procedures and illustrates the framework for multiple hypotheses testing in general parametric models. It is suitable for readers with R experience but limited knowledge of multiple comparison procedures and vice versa."--Publisher's description},
  publisher = {{CRC Press}},
  author = {Bretz, Frank and Hothorn, Torsten and Westfall, Peter H.},
  year = {2011},
  keywords = {Multiple comparisons (Statistics),R (Computer program language)},
  note = {OCLC: ocn643322735}
}


